---
layout: post
title:  "맵리듀스 작동 방법"
date:   2019-04-16 00:40:21
categories: [DataScience]
comments: true
---


# Chapter 7. 맵리듀스 작동 방법
## 7.1 맵리듀스 잡 실행 상세분석 

**맵리듀스 잡의 전체 과정**

![image](https://user-images.githubusercontent.com/28076434/56142959-904d3900-5fda-11e9-8acc-007fbc29e789.png)
job 객체의 submit() 메서드 호출로 맵리듀스 잡 실행 가능 (그림의 1번 과정) 
* 다섯 개의 독립적인 단계
1. 클라이언트 
-맵리듀스 잡을 제출
2. YARN 리소스 매니저 
-클러스터 상에 계산 리소스의 할당을 제어함
3. YARN 노드 매니저 
-클러스터의 각 머신에서 계산 컨테이너를 시작하고 모니터링함
4. 맵리듀스 애플리케이션 마스터 
-맵리듀스 잡을 수행하는 각 태스크를 제어함
-애플리케이션 마스터와 맵리듀스 태스크는 컨테이너 내에서 실행됨
-리소스 매니저는 잡을 할당하고 노드 매니저는 태스크를 관리함  
5. 분산 파일 시스템
-다른 단계 간에 잡 리소스 파일들을 공유함 (보통 HDFS 사용)


#### 7.1.1 잡 제출
* submit() 메서드는 내부의 JobSubmitter 인스턴스를 생성
* submitJobInternal() 메서드를 호출
* JobSubmitter의 잡 제출 과정
1. 리소스 매니저에 맵리듀스 잡 ID로 사용될 새로운 애플리케이션 ID를 요청
2. 잡의 출력 명세 확인(출력 디렉터리가 지정되지 않았거나 이미 존재한다면 해당 잡은 제출되지 않고 MR프로그램에 에러를 전달함)
3. 잡의 입력 스플릿을 계산(스플릿을 계산할 수 없다면 MR프로그램에 에러를 전달함)
4. 잡 실행에 필요한 잡 JAR파일, 환경 설정 파일, 계산된 입력 스플릿 등의 잡 리소스를 HDFS와 같은 공유 파일 시스템에 있는 해당 잡 ID 이름의 디렉터리에 복사함
5. 리소스 매니저의 submitApplication()을 호출하여 잡을 
* 일단 잡을 제출하면 waitForCompletion() 메서드가 1초에 한 번씩 잡의 진행상황을 조사, 변경 내역은 콘솔로 보여줌
* 잡이 성공적으로 완료되면 잡 카운터를 보여줌

#### 7.1.2 잡 초기화
1. 리소스 매니저가 submitApplication() 메서드의 호출을 받으면 YARN 스케줄러에 요청을 전달함
2. 스케줄러는 컨테이너를 하나 할당하고, 리소스 매니저는 노드 매니저의 운영 규칙에 따라 애플리케이션 마스터 프로세스를 시작함
**애플리케이션 마스터**
* 자바 애플리케이션이며 메인 클래스는 MRAppMaster
* 잡을 초기화할 때 잡의 진행 상태를 추적하기 위한 다수의 북키핑 객체를 생성 후, 이후 각 태스크로부터 진행 및 종료 보고서를 받음
* 클라이언트가 계산한 입력 스플릿 정보를 공유 파일시스템에서 읽어옴
* 입력 스플릿 별로 맵 태스크 객체를 생성하고 mapreduce.job.reduces 속성의 값(리듀서 수)만큼 객체를 생성하여 각 태스크는 ID를 부여받음
* 잡을 구성하는 태스크를 실행하는 방법을 결정해야 함
-잡의 크기가 작다면 자신의 JVM에서 실행할 수도 있음. 이러한 잡을 우버태스크로 실행된다고 함 
-우버 태스크 : 애플리케이션 마스터 내의 맵리듀스에서 실행되는 것으로 별도의 컨테이너를 생성하지 않아 오버헤드를 줄일 수 있음 
-작은 잡이란 보통 10개 미만의 매퍼와 하나의 리듀서, HDFS 블록 하나보다 작은 크기의 입력을 말함 
-우버 태스크는 반드시 mapreduce.job.ubertask.enable속성을 true로 변경하여 활성화해야 함
* 태스크를 실행하기 전 OutputCommitter의 setupJob() 메서드를 호출하여 잡의 최종 출력 디렉터리와 태스크 출력을 위한 임시 작업 공간을 생성함

#### 7.1.3 태스크 할당
1. 잡을 우버 태스크로 실행하기 적합하지 않다면 애플리케이션 마스터는 리소스 매니저에 잡의 모든 맵과 리듀스 태스크를 위한 컨테이너를 요청함(우선 순위 : 맵 태스크 > 리듀스 태스크)
2. 리듀스 태스크는 클러스터의 어느 곳에서도 실행 가능하지만 맵 태스크 요청은 데이터 지역성 제약이 있음
3. 맵 태스크는 각 태스크 트래커와 인접한 입력 스플릿을 갖도록 태스크가 할당됨
* 최적화 정도 : 태스크와 입력 스플릿이 동일 노드에 존재할 때(데이터 로컬) > 동일 랙에 존재할 때(렉 로컬) > 태스크가 다른 노드에서 입력 스플릿을 조회할 때
4. 요청할 때 태스크를 위한 메모리 요구사항과 CPU 수를 명시함(기본적으로 1024MB의 메모리와 가상 코어 1개를 각 맵과 리듀스 태스크에 할당)

#### 7.1.4 태스크 실행
1. 리소스 매니저의 스케줄러가 특정 노드 상의 컨테이너를 위한 리소스를 태스크에 할당하면 애플리케이션 마스터는 노드 매니저와 통신하며 컨테이너를 시작함
2. 각 태스크는 YarnChild 메인 클래스를 가진 자바 애플리케이션으로 실행됨
3. 공유파일시스템에 저장되어 있는 잡 환경 설정, Jar파일, 분산 캐시와 관련된 파일 등의 리소스를 로컬로 가져와야 함
4. 최종적으로 맵과 리듀스 태스크를 실제 실행함
* YarnChild는 전용 JVM에서 실행되므로 사용자 정의 맵과 리듀스 함수에서 버그가 발생하여 강제 종료 혹은 멈추어도 노드 매니저는 영향을 받지 않음
* 각 태스크는 태스크 자체외 동일한 JVM에서 설정과 커밋 동작을 수행, 잡의 OutputCommitter가 이를 결정함
* 파일 기반의 잡에서 커밋 동작은 임시 위치에서 최종 위치로 태스크 출력을 옮김
* 커밋 프로토콜은 투기적 실행히 활성화되었을 때 중복 태스크 중 단 하나만 커밋하고 나머지는 버림(뒤에 등장)

**스트리밍**
![image](https://user-images.githubusercontent.com/28076434/56286160-fd380e80-6153-11e9-8d77-063890931665.png)
* 스트리밍은 사용자가 제공한 실행파일을 시작하고 이와 통신하기 위한 목적을 가진 특별한 맵과 리듀스 태스크를 실행함
* 표준 입출력 스트림을 통해 프로세스와 통신함
1. 자바 프로세스는 키-값 쌍을 외부 프로세스에 전달
2. 사용자 정의 맵과 리듀스 함수로 처리
3. 출력 키-값 쌍을 자바 프로세스에 돌려줌
-> 노드 매니저 관점에서는 자식 프로세스가 맵과 리듀스 코드를 스스로 실행한 것처럼 보임

#### 7.1.5 진행 상황과 상태 갱신
![image](https://user-images.githubusercontent.com/28076434/56228213-38820100-60b2-11e9-9ceb-8b08b1982094.png)
* 맵리듀스 잡은 수행 시간이 오래걸리는 배치 잡이므로 사용자가 잡의 진행 상황에 대한 피드백을 받는 것이 매우 중요함
* 잡과 태스크는 실행중, 완료됨, 실패와 같은 상태, 맵과 리듀스의 진행상황, 명세 등의 상태 정보를 가짐
* 태스크가 수행되는 동안 태스크는 자신의 진행 상황을 추적함
-맵 태스크 : 처리한 입력 데이터의 비율
-리듀스 태스크 : 리듀스가 처리한 입력 데이터의 비율
* 전체 진행 과정을 총 세 부분으로 나눔(셔플의 세 단계와 관련있음)
-책 예시 : 리듀서에서 입력의 절반을 처리한 경우, 진행 상황은 5/6 (복사와 정렬이 각각 1/3, 리듀스의 절반 1/6 진행)
* 맵리듀스가 진행 중임을 판단하는 동작
하둡은 진행 중인 태스크는 실패로 보지 않음. 따라서 진행상황을 보고하는 것이 매우 중요함.
1. 입력 레코드 읽기(매퍼 또는 리듀서에서)
2. 출력 레코드 쓰기(매퍼 또는 리듀서에서)
3. 상태 명세의 설정(Reporter 또는 TaskAttemptContext의 setStatus() 메서드를 통해)
4. 카운터 증가(Reporter의 incrCounter() 메서드 또는 Counter의 increment() 메서드를 사용하여)
5. Reporter 또는 TaskAttempContext의 progress() 메서드 호출

* 태스크는 수행 중 발생하는 다양한 이벤트를 세는 여러 카운터를 가짐
* 맵과 리듀스 태스크가 실행되면서 자식 프로세스는 부모인 애플리케이션 마스터와 밀접한 인터페이스를 통해 통신함
-태스크는 진행상황과 상태 정보(카운터 포함)를 집계하여 매 3초마다 인터페이스를 통해 애플리케이션 마스터에 보고함
* 리소스 매니저 웹 UI는 실행 중인 모든 애플리케이션 각각의 애플리케이션 마스터 웹 UI 링크를 보여줌
* 잡이 진행되는 동안 클라이언트는 매초마다 애플리케이션 마스터를 폴링하여 가장 최근의 상태를 받으며 Job의 getStatus() 메서드를 이용해 JobStatus 인스턴스(잡의 모든 상태 정보)를 얻을 수 있음

#### 7.1.6 잡 완료
1. 애플리케이션 마스터가 마지막 태스크가 완료되었다는 통지를 발ㄷ으면 잡의 상태를 성공으로 변경
2. 사용자에게 통지할 메시지를 출력함
3. waitForCompletion() 메서드가 반환되며 잡 통계와 카운터가 콘솔에 출력됨
* HTTP 잡 통지를 보내도록 설정되어 있다면 애플리케이션 마스터는 이를 수행함
4. 잡이 완료되면 애플리케이션 마스터와 태스크 컨테이너는 작업 상태를 정리하고 OutputCommitter의 commitJob() 메서드를 호출함

## 7.2 실패
실제 환경에서는 사용자 코드의 버그 때문에 프로세스가 강제로 죽거나 서버에 장애가 발생하는 일이 빈번함

#### 7.2.1 태스크 실패
1. 태스크 내 에러 발생 : 가장 흔한 실패 유형으로 맵 또는 리듀스 태스크 내 사용자 코드에서 런타임 예외를 던질 때
* 예외 발생 시 JVM은 애플리케이션 마스터에 에러를 보고함
* 애플리케이션 마스터는 이 태스크 시도를 실패로 표시하고 해당 리소스를 다른 태스크에서 사용 가능하도록 컨테이너를 풀어줌
* 스트리밍 태스크에서는 스트리밍 프로세스가 0이 아닌 코드를 반환하면 실패로 표기함
2. 태스크 JVM의 갑작스러운 종료
3. 행 걸린 태스크(멈춘 태스크)
* 태스크 트래커가 mapred.task.timeout 속성에 설정된 시간 내에 태스크의 진행 상황을 보고 받지 못하면 실패로 인식
* 실행 시간이 긴 태스크는 절대로 실패로 표시되지 않으므로 태스크가 주기적으로 진행 상황을 확실히 보고하도록 하는 것이 좋음
4. 잡 트래커의 태스크 재스케줄링
애플리케이션 마스터는 태스크 시도 실패를 알게 되면 해당 태스크 실행을 다시 스케줄링 함
* 실패한 태스크 트래커(노드 매니저)에게는 실패한 태스크를 재할당하지 않음
* 기본적으로 어떤 태스크가 4번 이상 실패하면, 전체 잡 실패 처리
* mapred.map.max.attempts, mapred.reduce.max.attemps에 최대 시도 회수 설정 (기본 : 4로 설정)
* mapred.max.map.failures.percent, mapred.max.reduce.failures.percent으로 태스크 실패 허용 최대 비율을 설정하여 실패에도 불구하고 잡의 실행 결과를 사용하고 싶을 때 사용함
5. 태스크 시도의 강제 종료
투기적 중첩이나 실행 기반인 노드 매니저가 실패해서 애플리케이션 마스터가 해당 노드 매니저에서 실행되는 모든 태스크 시도를 실패로 표시할 때, 강제 종료가 발생함. 태스크 자체의 잘못이 아니므로 태스크 전체 시도 횟수에 포함되지는 않음

#### 7.2.2 애플리케이션 마스터 실패
애플리케이션 마스터가 실패할 경우, 몇번의 재시도가 일어남
* mapreduce.am.max-attempts 속성으로 시도 최대 횟수를 조절 가능함
* YARN은 클러스터에서 실행 중인 모든 YARN 애플리케이션 마스터에 대해 일괄적으로 최대 시도 횟수의 제한을 줄 수 있고 개별 애플리케이션은 이를 넘길 수 없음. 따라서 맵리듀스 애플리케이션 마스터의 시도 횟수를 늘리고 싶다면 클러스터의 YARN 설정 또한 증가시켜야 함
**복구 작업 방식**
* 애플리케이션 마스터는 주기적으로 리소스 매니저에 하트비트를 보냄
* 애플리케이션 마스터가 실패하면 리소스 매니저가 새로운 컨테이너에서 실행할 새로운 인스턴스를 생성함
* yarn.app.mapreduce.am.ob.recovery.enable을 false로 하면 비활성화됨

* 클라이언트는 진행 상황 보고를 위해 애플리케이션 마스터를 폴링함
=> 클라이언트는 애플리케이션 마스터가 실패했을 경우, 리소스  매니저에 새로운 애플리케잉션 마스터의 주소를 요청함

#### 7.2.3 노드 매니저 실패
* 리소스 매니저로의 하트 비트 전송이 없을 경우 실패로 간주함
=> 기본값은 10분이고 yarn.resourcemanager.nm.liveness-monitor.expiryinterval-ms 속성으로 밀리초 단위의 설정 가능
* 실패한 노드 매니저에서 수행 중인 애플리케이션 마스터나 태스크는 각각의 복구 매카니즘을 따름
* 애플리케이션의 실패 횟수가 높으면 블랙 리스트로 노드 매니저가 관리됨
=> 애플리케이션 마스터가 블랙 리스트를 관리함
=> 한 노드 매니저에서 네개 이상의 맵리듀스 태스크가 실패하면 다른 노드로 다시 스케줄링함
=> mapreduce.job.maxtaskfailures.per.tracker 잡 속성으로 이 한계값을 설정 가능함

#### 7.2.4 리소스 매니저 실패
* 리소스 매니저의 실패는 심각한 상황(리소스 매니저 없이는 잡이나 태스크 컨테이너가 실행될 수 없음)
* 고가용성을 위해 두 개의 리소스 매니저를 활성대기 설정으로 실행해야 함
* 실행 중인 애플리케이션에 대한 모든 정보는 고가용 상태 저장소(주키퍼 혹은 HDFS)에 보관 되므로 대기 리소스 매니저로 실패한 활성 리소스 매니저의 핵심 상태 복구가 가능함
* 새로운 리소스 매니저가 시작되면 상태 저장소로부터 애플리케이션 정보를 읽고 클러스터에서 실행중인 모든 애플리케이션 마스터를 재시작함
* 장애극복 관리자가 대기 리소스 매니저에서 활성 리소스 매니저로의 전환을 담당함
* HDFS 고가용성과는 달리 장애극복 관리자는 반드시 독립 프로세스일 필요는 없음. 편리한 환경설정을 위해 기본적으로 리소스 매니저에 포함됨
* 클라이언트와 노드 매니저는 리소스 매니저의 장애극복을 처리할 수 있도록 라운드 로빈 방식으로 활성 리소스 매니저를 찾기위해 각 리소스 매니저에 연결을 시도해야 함

## 7.3 셔플과 정렬
**셔플**
* 정렬을 수행하고 맵의 출력을 리듀서의 입력으로 전달하는 과정.
* 맵 리듀스 프로그램을 최적화할 때 중요한 튜닝 포인트
* 셔플은 코드 기반의 영역으로 현재 정제와 개선이 끊임없이 이루어지고 있음

#### 7.3.1 맵 부분
![image](https://user-images.githubusercontent.com/28076434/56292127-8d308500-6161-11e9-935e-ccd6c6c22ecf.png)
* 각 맵 태스크는 환형 구조의 메모리 버퍼를 가지고 있으며, 이곳에 데이터를 기록함
* 버퍼의 내용이 특정 한계치(기본은 80%)에 도달하면 백 그라운드 스레드는 디스크로 스필 시작
=> 디스크로 쓰기에 앞서 스레드는 먼저 데이터를 리듀서 수에 맞게 파티션으로 나눔
=> 각 파티션 내에서 인메모리의 키에 따라 정렬을 수행
=> 만약 컴바이너 함수가 존재하면 정렬의 출력에 대해 수행함(컴바이너 함수 수행은 맵출력을 축소하여 로컬 디스크에 쓰거나 리듀서에 전송할 데이터의 양을 줄임)
* 메모리 버퍼가 스필 한계에 도달할때 마다 새로운 스필 파일이 생성되고, 태스크가 완료되기 전 스필파일을 하나로 병합함
=> 최소 3개의 스필 파일이 존재할 경우 출력 파일을 쓰기 전 컨바인 잡을 수행함
* 맵 출력을 디스크에 쓰려는 시점에 압축하는 것은 리듀서로 전송할 데이터양을 줄일 수 있으므로 때론 좋은 방법
=> mapreduce.map.output.compress 속성을 true로 설정하면 활성화됨
* 출력파일의 파티션은 HTTP를 통해 리듀서에 전달됨

#### 7.3.2 리듀스 부분
**맵의 출력은 항상 로컬 디스크에 쓰이지만 리듀스의 출력은 그렇지 않음!**
* 리듀스 태스크는 클러스터 전반에 걸쳐 있는 여러개의 맵 태스크로부터 파티션을 가져와야 함
**복사 단계** 
: 리듀스 태스크는 각 맵 태스크의 출력이 끝나는 즉시 복사하기 시작함
=> 맵 출력이 작을 경우 리듀스 태스크 JVM 메모리에 복사
* 인메모리 버퍼나 맵 출력 수가 한계치에 도달하면 병합되어 디스크에 저장됨
=> 컴바이너가 지정되었다면 병합 도중 디스크에 쓰여질 데이터의 양을 줄이는 것이 가능함

**정렬 단계** 
: 모든 맵출력이 복사되는 시점에 리듀스 태스크는 맵 출력을 병합하고 정렬 순서를 유지함
=> 라운드 단위로 이루어짐
=> 예시 : 50개의 맵 출력이 존재하고 병합 계수가 10인 경우, 다섯개의 라운드로 각 라운드에서 10개의 파일을 하나로 병합하여 다섯 개의 중간 파일이 생성됨

**리듀스 단계**
위의 예시에서 다섯 개의 파일을 하나의 정렬된 파일로 병합하는 최종 라운드를 가지는 대신 리듀스 함수에 곧바로 전송하여 디스크 IO를 줄임

**효율적인 병합**
최종 라운드에서 병합 계수에 도달하기 위한 최소한의 파일을 병합하는 것이 궁극적인 목적!
![image](https://user-images.githubusercontent.com/28076434/56295363-a50b0780-6167-11e9-966b-044808a88567.png)
처음 라운드에서 파일 4개 병합 + 라운드 3개에서 파일 10개씩 병합 + 남아있는 6개의 파일 => 총 10개 파일이 최종 라운드에서 생성됨
=> 전체 라운드 수의 변경 없이 디스크에 쓰는 데이터양을 최소화하는 최적화 과정
=> 최종 라운드에서는 항상 리듀스로 바로 병합됨

* 리듀스 함수는 정렬된 출력 내 각각의 키에 대해서 호출이 되고 이 단계의 출력은 HDFS에 곧바로 기록됨

#### 7.3.3 설정 조정
맵리듀스 성능 향상을 위한 셔플을 튜닝할 수 있음
**맵 측면에서 튜닝 속성**
![image](https://user-images.githubusercontent.com/28076434/56295990-9f61f180-6168-11e9-8587-6d909d75d2d3.png)
**리듀스 측면에서 튜닝 속성**
![image](https://user-images.githubusercontent.com/28076434/56296054-b86aa280-6168-11e9-89a5-ba3ab178acb7.png)
![image](https://user-images.githubusercontent.com/28076434/56296091-c91b1880-6168-11e9-881f-fd382fcffe45.png)

* 일반적인 원칙은 셔플에 가능한 한 많은 메모리를 할당하는 것
=> 맵과 리듀스 함수가 동작하는데 충분한 메모리 확보가 필요
=> 맵과 리듀스 함수를 작성할 때 가능한 적은 메모리를 사용하도록 해야 함
* 맵 측면에서 보면 다수의 디스크 스필을 피하는것이 최고의 성능을 내는 방법
=> 맵 출력 크기 측정이 가능하다면 mapreduce.task.io.sort.* 속성을 설정하여 스필 횟수를 최소화 할 수 있음
* 리듀스 측변에서는 중간 데이터 전체가 메모리에 존재할 때 최고의 성능을 얻을 수 있음
=> 일반적으로 리듀스 함수에 모든 메모리를 예약해두기 때문에 이러한 일은 발생하지 않음
=> 리듀스 함수가 조금의 메모리만 요구할 때, mapreduce.reduce.merge.inmem.threshold는 0, mapreduce.reduce.input.buffer.percent는 1.0으로 설정하여 성능 향상에 기여 가능

## 7.4 태스크 실행
맵리듀스 사용자가 태스크 실행에 관해 취할 수 있는 제어사항에 대해 알아보자.
#### 7.4.1 태스크 실행 환경
